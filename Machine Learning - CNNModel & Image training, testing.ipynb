{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 導入函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random as r\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 對資料集內的各個資料夾取得圖片、進行降採樣、打亂資料，並且依據輸入比例分為訓練集和測試集，最後輸出圖片路徑和對應label list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(file_dir, train_size):\n",
    "\n",
    "    images = []   \n",
    "    subfolders = []\n",
    "    train_array = []\n",
    "    test_array = []\n",
    "    train_img_list = []\n",
    "    test_img_list = []\n",
    "    train_all_img = []\n",
    "    test_all_img = []\n",
    " \n",
    "    for dirPath, dirNames, fileNames in os.walk(file_dir):\n",
    "        \n",
    "        names = []\n",
    "        for name in fileNames:\n",
    "            names.append(os.path.join(dirPath, name))\n",
    "\n",
    "        for name in dirNames:\n",
    "            subfolders.append(os.path.join(dirPath, name))\n",
    "\n",
    "        r.shuffle(names)\n",
    "        if names != []:\n",
    "            images.append(names)\n",
    "\n",
    "    mincount = float(\"Inf\")\n",
    "    for num_folder in subfolders:\n",
    "        n_img = len(os.listdir(num_folder))\n",
    "        \n",
    "        if n_img < mincount:\n",
    "            mincount = n_img\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        images[i] = images[i][0:mincount]\n",
    "        train_img_list.append([])\n",
    "        test_img_list.append([])\n",
    "        train_num = math.ceil(len(images[i]) * train_size)\n",
    "        train_img_list[i] = images[i][0:train_num]\n",
    "        test_img_list[i] = images[i][train_num:]\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        train_all_img.extend(train_img_list[i])\n",
    "\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        test_all_img.extend(test_img_list[i])\n",
    "    \n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    for count in range(len(subfolders)):\n",
    "        train_labels = np.append(train_labels, len(train_img_list[0]) * [count])\n",
    "\n",
    "    for count in range(len(subfolders)):\n",
    "        test_labels = np.append(test_labels, len(test_img_list[0]) * [count])\n",
    "\n",
    "    train_array = np.array([train_all_img, train_labels])\n",
    "    train_array = train_array[:, np.random.permutation(train_array.shape[1])].T\n",
    "    test_array = np.array([test_all_img, test_labels])\n",
    "    test_array = test_array[:, np.random.permutation(test_array.shape[1])].T\n",
    "    \n",
    "    train_img = list(train_array[:, 0])\n",
    "    train_labels = list(train_array[:, 1])\n",
    "    train_labels = [int(float(i)) for i in train_labels]\n",
    "    \n",
    "    test_img = list(test_array[:, 0])\n",
    "    test_labels = list(test_array[:, 1])\n",
    "    test_labels = [int(float(i)) for i in test_labels]\n",
    "    \n",
    "\n",
    "    return train_img, train_labels, test_img, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 為了將各個圖片集轉為TFRecord檔，將每張圖片和其label轉為tf.train.Feature的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to TFRecords...\n",
      "Done\n",
      "Convert to TFRecords...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 轉Int64資料為 tf.train.Feature 格式\n",
    "def int64_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "# 轉Bytes資料為 tf.train.Feature 格式\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def convert_to_TFRecord(images, labels, name):\n",
    "    \n",
    "    filename = 'C:/SPLAB/TEST1/' + name + '.tfrecords'\n",
    "    \n",
    "    n_samples = len(labels)\n",
    "    if np.shape(images)[0] != n_samples:\n",
    "        raise ValueError('Images size {} does not match label size {}'.format(images.shape[0], n_samples))\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(filename)       # TFRecordWriter class\n",
    "    print ('Convert to TFRecords...')\n",
    "    for i in np.arange(0, n_samples):\n",
    "        try:\n",
    "            \n",
    "            image = cv2.imread(images[i], 0)                # type(image) must be array\n",
    "            image_raw = image.tostring()                  # transform array to bytes\n",
    "            label = int(labels[i])\n",
    "\n",
    "            ftrs = tf.train.Features(\n",
    "                    feature={'Label': int64_feature(label),\n",
    "                             'image_raw': bytes_feature(image_raw)}\n",
    "                   )\n",
    "    \n",
    "            example = tf.train.Example(features=ftrs)\n",
    "            writer.write(example.SerializeToString())\n",
    "        except IOError as e:\n",
    "            print ('Could not read:{}'.format(images[i]))\n",
    "            print ('Skip it!')\n",
    "    writer.close()\n",
    "    print ('Done')\n",
    "\n",
    "def main():\n",
    "    train_dataset_dir = 'C:/SPLAB/TEST1/DATA1'\n",
    "    train_img, train_labels, test_img, test_labels = get_files(train_dataset_dir,\n",
    "                                                               train_size=0.8)\n",
    "    \n",
    "    \n",
    "    # convert TFRecord file\n",
    "    TFRecord_list = ['train', 'test']\n",
    "    img_labels_list = [[train_img, train_labels], [test_img, test_labels]]\n",
    "\n",
    "    for index, TFRecord_name in enumerate(TFRecord_list):\n",
    "        convert_to_TFRecord(img_labels_list[index][0], img_labels_list[index][1],\n",
    "                            TFRecord_name)\n",
    "    #convert_to_TFRecord(images, labels, 'C:/SPLAB/TEST1/Train.tfrecords')\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將前面已轉為TFRecord檔的訓練集進行讀取和解碼成圖片與其標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename, batch_size, MAX_EPOCH): \n",
    "    # 建立文件名隊列\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs = MAX_EPOCH)\n",
    "    \n",
    "    # 數據讀取器\n",
    "    reader = tf.TFRecordReader()\n",
    "    key, values = reader.read(filename_queue)    # filename_queue\n",
    "    features = tf.parse_single_example(values,\n",
    "                                       features={\n",
    "                                            'Label': tf.FixedLenFeature([], tf.int64),\n",
    "                                            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                       })\n",
    "        # decode to tf.uint8\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)  # tf.uint8\n",
    "\n",
    "        # reshape\n",
    "    image = tf.reshape(image, [360, 210, 1])\n",
    "\n",
    "        # label\n",
    "    label = tf.cast(features['Label'], tf.int64)    # label tf.int32\n",
    "\n",
    "    image_batch, label_batch =tf.train.shuffle_batch(\n",
    "                                 [image, label],\n",
    "                                 batch_size=batch_size,\n",
    "                                 capacity=10000 + 3 * batch_size,\n",
    "                                 min_after_dequeue=1000)\n",
    "\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 為了建立卷積網路，先將權重、偏差、卷積和池化定義出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight(shape, mean=0, stddev=1):\n",
    "    init = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def bias(shape, mean=0, stddev=1):\n",
    "    init = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def conv2d(x, W, strides=[1, 1, 1, 1]):\n",
    "    return tf.nn.conv2d(x, W, strides=strides, padding='SAME')\n",
    "\n",
    "def max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1]):\n",
    "    return tf.nn.max_pool(x, ksize=ksize, strides=strides, padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 進行卷積網路的建立，並且決定使用 ReLU 函數作為 Activate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再選擇合適的優化器、輸入學習率來進行迭代，並且定義如何計算正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "filename = 'C:/SPLAB/TEST1/train.tfrecords'\n",
    "# batch 可以自由設定\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCH = 20\n",
    "LABEL_NUM = 5\n",
    "\n",
    "# Input images & labels\n",
    "X  = tf.placeholder(tf.float32, shape = [None, 360, 210, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, LABEL_NUM])\n",
    "\n",
    "# 卷積網路建立\n",
    "# Conv1\n",
    "W_conv1 = Weight([5, 5, 1, 16])\n",
    "b_conv1 = bias([16])\n",
    "y_conv1 = conv2d(X, W_conv1) + b_conv1\n",
    "# ReLU1\n",
    "relu1 = tf.nn.relu(y_conv1)\n",
    "# Pool1\n",
    "pool1 = max_pool(relu1)\n",
    "# Conv2\n",
    "W_conv2 = Weight([3, 3, 16, 32])\n",
    "b_conv2 = bias([32])\n",
    "y_conv2 = conv2d(pool1, W_conv2) + b_conv2\n",
    "# ReLU2\n",
    "relu2 = tf.nn.relu(y_conv2)\n",
    "# Pool2\n",
    "pool2 = max_pool(relu2)\n",
    "# FC1\n",
    "W_fc1 = Weight([90*53*32, 64])\n",
    "b_fc1 = bias([64])\n",
    "# 要把前面的網路的輸出壓扁成一維陣列\n",
    "h_flat = tf.reshape(pool2, [-1, 90*53*32])\n",
    "y_fc1 = tf.matmul(h_flat, W_fc1) + b_fc1\n",
    "# ReLU3\n",
    "relu3 = tf.nn.relu(y_fc1)\n",
    "# FC2 - 輸出層\n",
    "W_fc2 = Weight([64, 5]) # 分類0-9，共 10 類\n",
    "b_fc2 = bias([5])\n",
    "\n",
    "# 訓練用\n",
    "y = tf.matmul(relu3, W_fc2) + b_fc2\n",
    "saver = tf.train.Saver()\n",
    "# 預測用\n",
    "# 預測時，會先經過 softmax 計算，再取最大值為預測結果\n",
    "y_pred = tf.argmax(tf.nn.softmax(y), 1)\n",
    "\n",
    "# Cost optimizer\n",
    "lossFcn = tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "cost = tf.reduce_mean(lossFcn(labels=y_, logits=y))\n",
    "\n",
    "# 使用 AdamOptimizer 進行迭代\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# 計算正確率\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.add_to_collection('input' , X)\n",
    "tf.add_to_collection('output', y_pred)\n",
    "#tf.add_to_collection('prob', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開始進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-048e6a28556e>:3: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:113: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:2132: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-048e6a28556e>:6: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-4-048e6a28556e>:26: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-8942c18a58cc>:15: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Iter 0, accuracy 21.88%\n",
      "Iter 1, accuracy 23.44%\n",
      "Iter 2, accuracy 21.88%\n",
      "Iter 3, accuracy 14.06%\n",
      "Iter 4, accuracy 26.56%\n",
      "Iter 5, accuracy 10.94%\n",
      "Iter 6, accuracy 29.69%\n",
      "Iter 7, accuracy 18.75%\n",
      "Iter 8, accuracy 18.75%\n",
      "Iter 9, accuracy 18.75%\n",
      "Iter 10, accuracy 25.00%\n",
      "Iter 11, accuracy 20.31%\n",
      "Iter 12, accuracy 26.56%\n",
      "Iter 13, accuracy 20.31%\n",
      "Iter 14, accuracy 12.50%\n",
      "Iter 15, accuracy 28.12%\n",
      "Iter 16, accuracy 32.81%\n",
      "Iter 17, accuracy 46.88%\n",
      "Iter 18, accuracy 20.31%\n",
      "Iter 19, accuracy 25.00%\n",
      "Iter 20, accuracy 43.75%\n",
      "Iter 21, accuracy 37.50%\n",
      "Iter 22, accuracy 37.50%\n",
      "Iter 23, accuracy 39.06%\n",
      "Iter 24, accuracy 26.56%\n",
      "Iter 25, accuracy 39.06%\n",
      "Iter 26, accuracy 57.81%\n",
      "Iter 27, accuracy 42.19%\n",
      "Iter 28, accuracy 51.56%\n",
      "Iter 29, accuracy 67.19%\n",
      "Iter 30, accuracy 70.31%\n",
      "Iter 31, accuracy 51.56%\n",
      "Iter 32, accuracy 60.94%\n",
      "Iter 33, accuracy 62.50%\n",
      "Iter 34, accuracy 60.94%\n",
      "Iter 35, accuracy 57.81%\n",
      "Iter 36, accuracy 51.56%\n",
      "Iter 37, accuracy 51.56%\n",
      "Iter 38, accuracy 57.81%\n",
      "Iter 39, accuracy 67.19%\n",
      "Iter 40, accuracy 59.38%\n",
      "Iter 41, accuracy 73.44%\n",
      "Iter 42, accuracy 76.56%\n",
      "Iter 43, accuracy 71.88%\n",
      "Iter 44, accuracy 71.88%\n",
      "Iter 45, accuracy 76.56%\n",
      "Iter 46, accuracy 82.81%\n",
      "Iter 47, accuracy 64.06%\n",
      "Iter 48, accuracy 71.88%\n",
      "Iter 49, accuracy 70.31%\n",
      "Iter 50, accuracy 59.38%\n",
      "Iter 51, accuracy 79.69%\n",
      "Iter 52, accuracy 78.12%\n",
      "Iter 53, accuracy 76.56%\n",
      "Iter 54, accuracy 76.56%\n",
      "Iter 55, accuracy 81.25%\n",
      "Iter 56, accuracy 87.50%\n",
      "Iter 57, accuracy 75.00%\n",
      "Iter 58, accuracy 75.00%\n",
      "Iter 59, accuracy 87.50%\n",
      "Iter 60, accuracy 78.12%\n",
      "Iter 61, accuracy 76.56%\n",
      "Iter 62, accuracy 73.44%\n",
      "Iter 63, accuracy 81.25%\n",
      "Iter 64, accuracy 71.88%\n",
      "Iter 65, accuracy 93.75%\n",
      "Iter 66, accuracy 89.06%\n",
      "Iter 67, accuracy 84.38%\n",
      "Iter 68, accuracy 90.62%\n",
      "Iter 69, accuracy 78.12%\n",
      "Iter 70, accuracy 90.62%\n",
      "Iter 71, accuracy 85.94%\n",
      "Iter 72, accuracy 70.31%\n",
      "Iter 73, accuracy 71.88%\n",
      "Iter 74, accuracy 82.81%\n",
      "Iter 75, accuracy 89.06%\n",
      "Iter 76, accuracy 82.81%\n",
      "Iter 77, accuracy 85.94%\n",
      "Iter 78, accuracy 82.81%\n",
      "Iter 79, accuracy 98.44%\n",
      "Iter 80, accuracy 85.94%\n",
      "Iter 81, accuracy 89.06%\n",
      "Iter 82, accuracy 85.94%\n",
      "Iter 83, accuracy 93.75%\n",
      "Iter 84, accuracy 98.44%\n",
      "Iter 85, accuracy 78.12%\n",
      "Iter 86, accuracy 87.50%\n",
      "Iter 87, accuracy 89.06%\n",
      "Iter 88, accuracy 93.75%\n",
      "Iter 89, accuracy 93.75%\n",
      "Iter 90, accuracy 93.75%\n",
      "Iter 91, accuracy 93.75%\n",
      "Iter 92, accuracy 95.31%\n",
      "Iter 93, accuracy 95.31%\n",
      "Iter 94, accuracy 92.19%\n",
      "Iter 95, accuracy 90.62%\n",
      "Iter 96, accuracy 92.19%\n",
      "Iter 97, accuracy 93.75%\n",
      "Iter 98, accuracy 81.25%\n",
      "Iter 99, accuracy 90.62%\n",
      "Iter 100, accuracy 90.62%\n",
      "Iter 101, accuracy 89.06%\n",
      "Iter 102, accuracy 98.44%\n",
      "Iter 103, accuracy 92.19%\n",
      "Iter 104, accuracy 93.75%\n",
      "Iter 105, accuracy 87.50%\n",
      "Iter 106, accuracy 89.06%\n",
      "Iter 107, accuracy 93.75%\n",
      "Iter 108, accuracy 96.88%\n",
      "Iter 109, accuracy 93.75%\n",
      "Iter 110, accuracy 96.88%\n",
      "Iter 111, accuracy 90.62%\n",
      "Iter 112, accuracy 92.19%\n",
      "Iter 113, accuracy 89.06%\n",
      "Iter 114, accuracy 96.88%\n",
      "Iter 115, accuracy 85.94%\n",
      "Iter 116, accuracy 85.94%\n",
      "Iter 117, accuracy 98.44%\n",
      "Iter 118, accuracy 96.88%\n",
      "Iter 119, accuracy 96.88%\n",
      "Iter 120, accuracy 96.88%\n",
      "Iter 121, accuracy 95.31%\n",
      "Iter 122, accuracy 92.19%\n",
      "Iter 123, accuracy 98.44%\n",
      "Iter 124, accuracy 96.88%\n",
      "Iter 125, accuracy 96.88%\n",
      "Iter 126, accuracy 93.75%\n",
      "Iter 127, accuracy 89.06%\n",
      "Iter 128, accuracy 100.00%\n",
      "Iter 129, accuracy 96.88%\n",
      "Iter 130, accuracy 95.31%\n",
      "Iter 131, accuracy 100.00%\n",
      "Iter 132, accuracy 96.88%\n",
      "Iter 133, accuracy 98.44%\n",
      "Iter 134, accuracy 100.00%\n",
      "Iter 135, accuracy 96.88%\n",
      "Iter 136, accuracy 100.00%\n",
      "Iter 137, accuracy 100.00%\n",
      "Iter 138, accuracy 98.44%\n",
      "Iter 139, accuracy 100.00%\n",
      "Iter 140, accuracy 100.00%\n",
      "Iter 141, accuracy 96.88%\n",
      "Iter 142, accuracy 100.00%\n",
      "Iter 143, accuracy 98.44%\n",
      "Iter 144, accuracy 96.88%\n",
      "Iter 145, accuracy 98.44%\n",
      "Iter 146, accuracy 98.44%\n",
      "Iter 147, accuracy 96.88%\n",
      "Iter 148, accuracy 98.44%\n",
      "Iter 149, accuracy 92.19%\n",
      "Iter 150, accuracy 93.75%\n",
      "Iter 151, accuracy 95.31%\n",
      "Iter 152, accuracy 96.88%\n",
      "Iter 153, accuracy 98.44%\n",
      "Iter 154, accuracy 100.00%\n",
      "Iter 155, accuracy 95.31%\n",
      "Iter 156, accuracy 92.19%\n",
      "Iter 157, accuracy 92.19%\n",
      "Iter 158, accuracy 100.00%\n",
      "Iter 159, accuracy 98.44%\n",
      "Iter 160, accuracy 100.00%\n",
      "Iter 161, accuracy 100.00%\n",
      "Iter 162, accuracy 100.00%\n",
      "Iter 163, accuracy 100.00%\n",
      "Iter 164, accuracy 98.44%\n",
      "Iter 165, accuracy 98.44%\n",
      "Iter 166, accuracy 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 167, accuracy 98.44%\n",
      "Iter 168, accuracy 100.00%\n",
      "Iter 169, accuracy 96.88%\n",
      "Iter 170, accuracy 85.94%\n",
      "Iter 171, accuracy 98.44%\n",
      "Iter 172, accuracy 98.44%\n",
      "Iter 173, accuracy 100.00%\n",
      "Iter 174, accuracy 100.00%\n",
      "Iter 175, accuracy 100.00%\n",
      "Iter 176, accuracy 96.88%\n",
      "Iter 177, accuracy 95.31%\n",
      "Iter 178, accuracy 96.88%\n",
      "Iter 179, accuracy 93.75%\n",
      "Iter 180, accuracy 98.44%\n",
      "Iter 181, accuracy 90.62%\n",
      "Iter 182, accuracy 98.44%\n",
      "Iter 183, accuracy 93.75%\n",
      "Iter 184, accuracy 95.31%\n",
      "Iter 185, accuracy 100.00%\n",
      "Iter 186, accuracy 100.00%\n",
      "Iter 187, accuracy 93.75%\n",
      "Iter 188, accuracy 93.75%\n",
      "Iter 189, accuracy 95.31%\n",
      "Iter 190, accuracy 96.88%\n",
      "Iter 191, accuracy 98.44%\n",
      "Iter 192, accuracy 100.00%\n",
      "Iter 193, accuracy 100.00%\n",
      "Iter 194, accuracy 98.44%\n",
      "Iter 195, accuracy 92.19%\n",
      "Iter 196, accuracy 98.44%\n",
      "Iter 197, accuracy 100.00%\n",
      "Iter 198, accuracy 96.88%\n",
      "Iter 199, accuracy 95.31%\n",
      "Iter 200, accuracy 96.88%\n",
      "Iter 201, accuracy 100.00%\n",
      "Iter 202, accuracy 96.88%\n",
      "Iter 203, accuracy 98.44%\n",
      "Iter 204, accuracy 98.44%\n",
      "Iter 205, accuracy 100.00%\n",
      "Iter 206, accuracy 96.88%\n",
      "Iter 207, accuracy 100.00%\n",
      "Iter 208, accuracy 100.00%\n",
      "Iter 209, accuracy 98.44%\n",
      "Iter 210, accuracy 100.00%\n",
      "Iter 211, accuracy 98.44%\n",
      "Iter 212, accuracy 100.00%\n",
      "Iter 213, accuracy 100.00%\n",
      "Iter 214, accuracy 96.88%\n",
      "Iter 215, accuracy 98.44%\n",
      "Iter 216, accuracy 100.00%\n",
      "Iter 217, accuracy 100.00%\n",
      "Iter 218, accuracy 98.44%\n",
      "Iter 219, accuracy 100.00%\n",
      "Iter 220, accuracy 100.00%\n",
      "Iter 221, accuracy 100.00%\n",
      "Iter 222, accuracy 98.44%\n",
      "Iter 223, accuracy 100.00%\n",
      "Iter 224, accuracy 98.44%\n",
      "Iter 225, accuracy 98.44%\n",
      "Iter 226, accuracy 100.00%\n",
      "Iter 227, accuracy 98.44%\n",
      "Iter 228, accuracy 100.00%\n",
      "Iter 229, accuracy 98.44%\n",
      "Iter 230, accuracy 98.44%\n",
      "Iter 231, accuracy 100.00%\n",
      "Iter 232, accuracy 98.44%\n",
      "Iter 233, accuracy 96.88%\n",
      "Iter 234, accuracy 100.00%\n",
      "Iter 235, accuracy 95.31%\n",
      "Iter 236, accuracy 100.00%\n",
      "Iter 237, accuracy 100.00%\n",
      "Iter 238, accuracy 92.19%\n",
      "Iter 239, accuracy 96.88%\n",
      "Iter 240, accuracy 96.88%\n",
      "Iter 241, accuracy 100.00%\n",
      "Iter 242, accuracy 98.44%\n",
      "Iter 243, accuracy 100.00%\n",
      "Iter 244, accuracy 100.00%\n",
      "Iter 245, accuracy 100.00%\n",
      "Iter 246, accuracy 100.00%\n",
      "Iter 247, accuracy 100.00%\n",
      "Iter 248, accuracy 100.00%\n",
      "Iter 249, accuracy 100.00%\n",
      "Iter 250, accuracy 100.00%\n",
      "Iter 251, accuracy 100.00%\n",
      "Iter 252, accuracy 100.00%\n",
      "Done!\n",
      "Model saved in file: C:/SPLAB/TEST1/log1/test_model\n"
     ]
    }
   ],
   "source": [
    "# 設定輸入資料來源\n",
    "img_bat, lb_bat = read_and_decode(filename, BATCH_SIZE, MAX_EPOCH)\n",
    "train_x = tf.reshape(img_bat, [-1, 360, 210, 1])\n",
    "train_y = tf.one_hot(lb_bat, LABEL_NUM)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # 變數初始化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    # 啟動 TFRecord 佇列\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    i = 0\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            image_train, label_train = sess.run([train_x, train_y])\n",
    "            \n",
    "            sess.run(train_step, feed_dict={ X : image_train, \n",
    "                                             y_: label_train})\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={ X  : image_train, \n",
    "                                                                y_ : label_train})\n",
    "\n",
    "                print('Iter %d, accuracy %4.2f%%' % (i,train_accuracy*100))\n",
    "                #print (sess.run(cost))\n",
    "            i += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done!')\n",
    "        \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "            \n",
    "    coord.join(threads)\n",
    "    \n",
    "    # 存檔路徑 #\n",
    "    save_path = 'C:/SPLAB/TEST1/log1/test_model'\n",
    "\n",
    "    # 把整張計算圖存檔\n",
    "    spath = saver.save(sess, save_path)\n",
    "    print(\"Model saved in file: %s\" % spath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 為了要使用 混淆矩陣 而導入函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將前面已轉為TFRecord檔的測試集進行讀取和解碼成圖片與其標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decode(filename, batch_size): \n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs = 1)\n",
    "    \n",
    "    # 數據讀取器\n",
    "    reader = tf.TFRecordReader()\n",
    "    key, values = reader.read(filename_queue)    # filename_queue\n",
    "    features = tf.parse_single_example(values,\n",
    "                                       features={\n",
    "                                            'Label': tf.FixedLenFeature([], tf.int64),\n",
    "                                            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                       })\n",
    "        # decode to tf.uint8\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)  # tf.uint8\n",
    "\n",
    "        # reshape\n",
    "    image = tf.reshape(image, [360, 210, 1])\n",
    "\n",
    "        # label\n",
    "    label = tf.cast(features['Label'], tf.int64)    # label tf.int32\n",
    "    \n",
    "    image_batch, label_batch =tf.train.batch(\n",
    "                                 [image, label],\n",
    "                                 batch_size=batch_size,\n",
    "                                 capacity=10000 + 3 * batch_size)\n",
    "\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-4b5f66b10a89>:25: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "filename = 'C:/SPLAB/TEST1/test.tfrecords'\n",
    "\n",
    "LABEL_NUM = 5\n",
    "\n",
    "pre_img_list = []\n",
    "rit_lbl_list = []\n",
    "\n",
    "test_img, test_lbl = test_decode(filename, 1)\n",
    "test_x = tf.reshape(test_img, [-1, 360, 210, 1])\n",
    "test_y = tf.one_hot(test_lbl, LABEL_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開始進行測試，並且輸出混淆矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:/SPLAB/TEST1/log1/test_model\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 1\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 3\n",
      "test:\n",
      "[3]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 0\n",
      "test:\n",
      "[0]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 1\n",
      "test:\n",
      "[1]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 4\n",
      "test:\n",
      "[4]\n",
      "label: 2\n",
      "test:\n",
      "[2]\n",
      "Done!\n",
      "[[40  0  0  0  0]\n",
      " [ 0 39  0  0  1]\n",
      " [ 0  0 40  0  0]\n",
      " [ 0  0  0 40  0]\n",
      " [ 0  0  0  1 39]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # 變數初始化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    # 啟動 TFRecord 佇列\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    save_path = 'C:/SPLAB/TEST1/log1/test_model.meta'\n",
    "    saver = tf.train.import_meta_graph(save_path)\n",
    "    saver.restore(sess, 'C:/SPLAB/TEST1/log1/test_model')\n",
    "    \n",
    "    x = tf.get_collection(\"input\")[0]\n",
    "    y = tf.get_collection(\"output\")[0]\n",
    "\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            image_test, label_test = sess.run([test_x, test_y])\n",
    "            label_ = np.argmax(label_test, 1) \n",
    "            for j in np.arange(1):\n",
    "                print ('label: {}'.format(label_[j]))\n",
    "                rit_lbl_list.append(label_[j])\n",
    "                result = sess.run(y, feed_dict = { x: image_test})             \n",
    "                pre_img_list.append(result)\n",
    "                print(\"test:\")\n",
    "                print(result)\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done!')\n",
    "        \n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "            \n",
    "    coord.join(threads)\n",
    "\n",
    "    C = confusion_matrix(rit_lbl_list, pre_img_list)\n",
    "    print (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
